{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import deeprl.infrastructure.pytorch_util as ptu\n",
        "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
        "from deeprl.infrastructure.trainers import AC_Trainer\n",
        "\n",
        "from deeprl.policies.MLP_policy import MLPPolicyAC\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x,y):\n",
        "  return np.max(np.abs(x-y)/ (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "def remove_folder(path):\n",
        "  if os.path.exists(path):\n",
        "    print(\"clearing old results at {}\".format(path))\n",
        "    shutil.rmtree(path)\n",
        "  else:\n",
        "    print(\"folder {} does not exist yet. no old results to delete\".format(path))\n",
        "\n",
        "\n",
        "ac_base_args_dict = dict(\n",
        "   env_name = 'hopper-v2',\n",
        "   exp_name = 'test_ac'\n",
        "   save_params = False,\n",
        "\n",
        "   ep_len = 200,\n",
        "   discount = 0.99,\n",
        "\n",
        "   num_agent_train_steps_per_iter = 1000,\n",
        "   n_iter = 100,\n",
        "\n",
        "   batch_size = 1000,\n",
        "   eval_batch_size = 1000,\n",
        "   train_batch_size = 256,\n",
        "   max_replay_buffer_size = 1000000,\n",
        "\n",
        "   n_layers = 2,\n",
        "   size = 256,\n",
        "   entropy_weight = 0,\n",
        "   learning_rate = 3e-4,\n",
        "\n",
        "   critic_n_layers = 2,\n",
        "   critic_size = 256,\n",
        "   target_update_rate = 5e-3,\n",
        "\n",
        "   video_log_freq = -1,\n",
        "   scalar_log_freq = 1,\n",
        "\n",
        "   no_gpu = False,\n",
        "   which_gpu = 0,\n",
        "   seed = 2,\n",
        "   logdir = 'test',\n",
        ")\n",
        "\n",
        "ac_dim = 3\n",
        "ob_dim = 11\n",
        "N = 5\n",
        "\n",
        "np.random.seed(0)\n",
        "obs = np.random.normal(size=(N, ob_dim))\n",
        "acts = np.random.choice(ac_dim, size=(N,))\n",
        "next_obs = np.random.normal(size=(N, ob_dim))\n",
        "rewards = np.random.normal(size=N)\n",
        "terminals = np.zero(N)\n",
        "terminals[0] = 1\n",
        "\n",
        "ac_args = dict(ac_base_args_dict)\n",
        "\n",
        "env_str = 'Hooper'\n",
        "ac_args['env_name'] = '{}- v2'.format(env_str)\n",
        "ac_args['entropy_weight'] = 0.1\n",
        "actrainer = AC_Trainer(ac_args)\n",
        "critic = actrainer.rl_trainer.agent.critic\n",
        "\n",
        "class DummyDist:\n",
        "  def sample(self):\n",
        "    return ptu.from_numpy(1 + np.zeros(shape=(N, ac_dim)))\n",
        "\n",
        "\n",
        "  def dummy_actors(next_obs):\n",
        "    return DummyDist()\n",
        "\n",
        "target_vals = critic.compute_target_value(ptu.from_numpy(next_obs),\n",
        "                                          ptu.from_numpy(rewards),\n",
        "                                          ptu.from_numpy(terminals),\n",
        "                                          dummy_actor)\n",
        "target_vals = ptu.to_numpy(target_vals)\n",
        "expected_targets = np.array([-0.9167948, -0.11123351, -0.36787638, -2.1131861,  -0.13868617])\n",
        "\n",
        "target_error = rel_error(target_vals, expected_targets)\n",
        "print(\"target value error\",target_error, \"should be on the order of 1e-6 or lower\")\n",
        "\n",
        "\n",
        "ac_args = dict(ac_base_args_dict)\n",
        "env_str = 'Hopper'\n",
        "ac_args['env_name'] = '{}- v2'.format(env_str)\n",
        "ac_args['entropy_weight'] = 0.1\n",
        "actrainer = AC_Trainer(ac_args)\n",
        "critic = actrainer.rl_trainer.agent.critic\n",
        "\n",
        "critic.target_update_rate = 0.5\n",
        "\n",
        "for p in critic.critic_network.parameters():\n",
        "  p.data += 1.\n",
        "\n",
        "critic.update_target_network_ema()\n",
        "\n",
        "for p, target_p in zip(critic.critic_network.parameters(), critic.target_network.parameters()):\n",
        "  assert np.all(ptu.to_numpy((p-target_p)) == 0.5)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "ac_dim = 2\n",
        "ob_dim = 3\n",
        "batch_size = 5\n",
        "\n",
        "np.random.seed(0)\n",
        "obs = np.random.normal(size=(N, ob_dim))\n",
        "\n",
        "policy = MLPPolicyAC(\n",
        "            ac_dim = ac_dim,\n",
        "            ob_dim = ob_dim,\n",
        "            n_layers = 1,\n",
        "            size = 2,\n",
        "            learning_rate = 0.25,\n",
        "            entropy_weight = 0.)\n",
        "\n",
        "def dummy_critic(obs, acts):\n",
        "  return torch.sum(acts + 1) + torch.sum(obs)\n",
        "\n",
        "initial_loss = policy.update(obs, dummy_critic)['Actor Training Loss']\n",
        "expected_initial_loss = -17.083496\n",
        "\n",
        "print(\"Initial loss error\", rel_error(expected_initial_loss, initial_loss), \"should be on the order of 1e-6 or less.\")\n",
        "for i in range(5):\n",
        "  loss = policy.update(obs, dummy_critic)['Actor Training Loss']\n",
        "  print(loss)\n",
        "\n",
        "expected_final_loss = -30.103575\n",
        "\n",
        "print(\"Final loss error\", rel_error(expected_final_loss, loss), \"should be on the order of 1e-6 or less.\")\n",
        "\n",
        "ac_args = dict(ac_base_args_dict)\n",
        "env_str = 'HalfCheetah'\n",
        "ac_args['env_name'] = '{}- v2'.format(env_str)\n",
        "ac_args['n_iter'] = 50\n",
        "\n",
        "remove_folder('logs/actor_critic/{}'.format(env_str))\n",
        "\n",
        "for seed in range(3):\n",
        "  print(\"Running actor critic experiment with seed\", seed)\n",
        "  ac_args['seed'] = seed\n",
        "  ac_args['logdir'] = 'logs/actor_critic/{}/seed{}'.format(env_str, seed)\n",
        "  actrainer = AC_Trainer(ac_args)\n",
        "  actrainer.run_training_loop()\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/actor_critic/HalfCheetah\n"
      ],
      "metadata": {
        "id": "mio_nCaj8Z3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}