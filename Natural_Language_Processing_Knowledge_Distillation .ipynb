{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQGmRQeEls7R"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE = True"
      ],
      "metadata": {
        "id": "BSD7HQaMmz2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "IxQw4TkZm1H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"\" if not DRIVE else \"/content/drive/My Drive/.../\"\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(root_folder)\n",
        "import json\n",
        "from utils import validate_to_array, model_out_to_list\n",
        "import torch as th\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import math\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertForPreTraining"
      ],
      "metadata": {
        "id": "yYpC6uo-m8GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "teacher_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "6tB6_qnunyzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(teacher_model)"
      ],
      "metadata": {
        "id": "HJaV5LeVoJj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_kwargs = dict(\n",
        "\n",
        "  return_dict=True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "JrWPF5DAoMiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(teacher_model.forward)"
      ],
      "metadata": {
        "id": "4gj5_DZlo1MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import AttentionLayerLoss\n",
        "num_channels = 10\n",
        "batch_size = 2\n",
        "\n",
        "with open(root_folder+\"kd_checks/kd_attention_loss.json\", 'r') as f:\n",
        "  io = json.load(f)\n",
        "  teacher_attn = th.tensor(io['teacher_attention'])\n",
        "  student_attn = th.tensor(io['student_attention'])\n",
        "  expected_output = th.tensor(io['expected_output'])\n",
        "\n",
        "attn_loss = AttentionLayerLoss()\n",
        "output = attn_loss(teacher_attn, student_attn)\n",
        "validate_to_array(model_out_to_list, ((teacher_attn, student_attn), attn_loss)'kdattnloss', root_folder)\n",
        "print(\"Total error on the output:\", th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "metadata": {
        "id": "h8M7TT1wo3_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import HiddenLayerLoss\n",
        "teacher_hidden_dim = 50\n",
        "student_hidden_dim = 10\n",
        "batch_size = 2\n",
        "\n",
        "with open(root_folder+\"kd_checks/kd_hidden_loss.json\", 'r') as f:\n",
        "  io = json.load(f)\n",
        "  teacher_hddn = th.tensor(io['teacher_hidden'])\n",
        "  student_hddn = th.tensor(io['student_hidden'])\n",
        "  expected_output = th.tensor(io['expected_output'])\n",
        "\n",
        "hddn_loss = HiddenLayerLoss(teacher_hidden_dim,student_hidden_dim)\n",
        "hddn_loss.load_state_dict(th.load(root_folder+\"kd_checks/kd_hidden_loss\"))\n",
        "output = hddn_loss(teacher_hddn, student_hddn)\n",
        "validate_to_array(model_out_to_list, ((teacher_hddn,student_hddn), hddn_loss), 'kdhddnloss', root_folder)\n",
        "print(\"Total error on the output:\", th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "metadata": {
        "id": "RJqzlr9jqpEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import EmbeddingLayerLoss\n",
        "teacher_embed_dim = 50\n",
        "student_embed_dim = 10\n",
        "batch_size = 2\n",
        "\n",
        "with open(root_folder+\"kd_checks/kd_embed_loss.json\", 'r') as f:\n",
        "  io = json.load(f)\n",
        "  teacher_embd = th.tensor(io['teacher_embed'])\n",
        "  student_embd = th.tensor(io['student_embed'])\n",
        "  expected_output = th.tensor(io['expected_output'])\n",
        "\n",
        "embd_loss = EmbeddingLayerLoss(teacher_embed_dim,student_embed_dim)\n",
        "embd_loss.load_state_dict(th.load(root_folder+\"kd_checks/kd_embed_loss\"))\n",
        "output = embd_loss(teacher_embd, student_embd)\n",
        "validate_to_array(model_out_to_list, ((teacher_embd,student_embd), embd_loss), 'kdembdloss', root_folder)\n",
        "print(\"Total error on the output:\", th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "metadata": {
        "id": "-sVBRjuNtrEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import PredictionLoss\n",
        "word_count = 10\n",
        "batch_size = 2\n",
        "\n",
        "with open(root_folder+\"kd_checks/kd_pred_loss.json\", 'r') as f:\n",
        "  io = json.load(f)\n",
        "  teacher_pred = th.tensor(io['teacher_pred'])\n",
        "  student_pred = th.tensor(io['student_pred'])\n",
        "  expected_output = th.tensor(io['expected_output'])\n",
        "\n",
        "pred_loss = PredictionLoss()\n",
        "output = pred_loss(teacher_pred, student_pred)\n",
        "validate_to_array(model_out_to_list, ((teacher_pred,student_pred), pred_loss), 'kdpredloss', root_folder)\n",
        "print(\"Total error on the output:\", th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "metadata": {
        "id": "GZtIS_nAur0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import KnowledgeDistillationLoss\n",
        "num_channels = 12\n",
        "teacher_hidden_dim = 60\n",
        "student_hidden_dim = 15\n",
        "teacher_embed_dim = 60\n",
        "student_embed_dim = 15\n",
        "word_count = 10\n",
        "teacher_num_blocks = 6\n",
        "student_num_blocks = 2\n",
        "batch_size = 2\n",
        "layer_mapping = range(2,6,3)\n",
        "\n",
        "with open(root_folder+\"kd_checks/kd_loss.json\", 'r') as f:\n",
        "  io = json.load(f)\n",
        "  teacher_attn = io['teacher_out']\n",
        "  student_attn = io['student_out']\n",
        "  teacher_out = dict(\n",
        "      embeddings = th.tensor(teacher_out['embeddings']),\n",
        "      attentions = [th.tensor(o) for o in teacher_out['attentions']],\n",
        "      hidden_states = [th.tensor(o) for o in teacher_out['hidden_states']],\n",
        "      logits = th.tensor(teacher_out['embeddings'])\n",
        "  )\n",
        "  student_out = dict(\n",
        "      embeddings = th.tensor(student_out['embeddings']),\n",
        "      attentions = [th.tensor(o) for o in student_out['attentions']],\n",
        "      hidden_states = [th.tensor(o) for o in student_out['hidden_states']],\n",
        "      logits = th.tensor(student_out['embeddings'])\n",
        "  )\n",
        "  expected_output = th.tensor(io['expected_output'])\n",
        "\n",
        "kd_loss = KnowledgeDistillationLoss(teacher_embed_dim, student_embed_dim, teacher_hidden_dim,student_hidden_dim,layer_mapping)\n",
        "kd_loss.load_state_dict(th.load(root_folder+\"kd_checks/kd_loss\"))\n",
        "output = kd_loss(teacher_out, student_out)\n",
        "validate_to_array(model_out_to_list, ((teacher_out,student_out), kd_loss), 'kdloss', root_folder)\n",
        "print(\"Total error on the output:\", th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "metadata": {
        "id": "gAECuMbvzMDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ],
      "metadata": {
        "id": "6mHIDya_4aQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets)"
      ],
      "metadata": {
        "id": "f7t-aNlb4hvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\".join(datasets['train'][:100]['text']))"
      ],
      "metadata": {
        "id": "T7gyNs9q4zq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenized_datasets = datasets.map(lambda samples: tokenizer(samples['text']), batched = True, num_proc = 4, remove_columns = [\"text\"])"
      ],
      "metadata": {
        "id": "aTMC4rqE45lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "def group_texts(examples):\n",
        "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "  total_length = (total_length // block_size) * block_size\n",
        "  result = {\n",
        "      k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "      for k, t in concatenated_examples.items()\n",
        "  }\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result\n",
        ""
      ],
      "metadata": {
        "id": "VQ0ChNtF5f_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size= 1000,\n",
        "    num_proc=4,\n",
        "    load_from_cache_file=False\n",
        ")"
      ],
      "metadata": {
        "id": "33iMA0G09L60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "metadata": {
        "id": "SeyY_BtI9x7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets[\"train\"][1].keys()"
      ],
      "metadata": {
        "id": "i36UxzJf90oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = int(1e4)\n",
        "teacher_hddn_dim = # ...\n",
        "student_hddn_dim = # ...\n",
        "teacher_num_hddn_layers = # ...\n",
        "student_num_hddn_layers = # ...\n",
        "teacher_num_attn_heads = # ...\n",
        "student_num_attn_heads = # ...\n",
        "teacher_ff_dim = # ...\n",
        "student_ff_dim = # ...\n",
        "teacher_embd_dim = # ...\n",
        "student_embd_dim = # ...\n",
        "layer_mapping = range(,\n",
        "                      ,\n",
        "                      )\n",
        "\n",
        "student_config = BertConfig(\n",
        "    hidden_size = student_hddn_dim,\n",
        "    num_hidden_layers = student_num_hddn_layers,\n",
        "    num_attention_heads = student_num_attn_heads,\n",
        "    intermediate_size = student_ff_dim,\n",
        ")"
      ],
      "metadata": {
        "id": "8uYC2Ya494SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import KnowledgeDistillationLoss\n",
        "teacher_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "teacher_model.load_state_dict(th.load(root_folder+'bert_models/teacher_wikitext.pt'))\n",
        "student_model = BertForMaskedLM(student_config).to(device)\n",
        "criterion = KnowledgeDistillationLoss(teacher_embd_dim, student_embd_dim, teacher_hddn_dim,student_hddn_dim,layer_mapping).to(device)"
      ],
      "metadata": {
        "id": "lQuuoFgzDoMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "gc.collect()\n",
        "optimizer = optim.Adam(params=student_model.parameters(), lr = 5e-5, weight_decay=0.01)\n",
        "student_model.to(device)\n",
        "lr = 1e-4\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  lm_datasets[\"train\"].shuffle(load_from_cache_file=False)\n",
        "  t = tqdm(range(0, len(lm_datasets[\"train\"]), batch_size))\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for i in t:\n",
        "    data = lm_datasets[\"train\"][i : i + batch_size]\n",
        "    data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "    teacher_out = teacher_model(**data, **forward_kwargs)\n",
        "    student_out = student_model(**data, **forward_kwargs)\n",
        "    teacher_out['embeddings'] = teacher_model.get_input_embeddings().weight\n",
        "    student_out['embeddings'] = student_model.get_input_embeddings().weight\n",
        "    loss = criterion(teacher_out, student_out, penalize_prediction=False)\n",
        "    losses.append(loss.detach().cpu().numpy())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    accuracy = th.eq(student_out['logits'].argmax(dim=2, keepdim = False).float(), data['labels']).float().mean()\n",
        "    accuracies.append(accuracy.detach().cpu().numpy())\n",
        "    loss = np.around(np.mean(losses[-100:]), 3)\n",
        "    accuracy = np.around(np.mean(accuracies[-100:]), 2)\n",
        "    t.set_description(\"Epoch: \"+str(epoch)+ \"Loss: \"+str(loss))\n",
        "  os.makedirs(root_folder+'bert_models', exist_ok=True)\n",
        "  th.save(student_model.state_dict(), root_folder+'bert_models/student_wikitext.pt')"
      ],
      "metadata": {
        "id": "HmDrh_n1FOOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "control_model = BertForMaskedLM(student_config).to(device)\n",
        "gc.collect()\n",
        "control_model.train()\n",
        "optimizer = optim.Adam(params=student_model.parameters(), lr = lr, weight_decay=0.01)\n",
        "lr = 1e-4\n",
        "batch_size = 10\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  lm_datasets[\"train\"].shuffle()\n",
        "  t = tqdm(range(0, len(lm_datasets[\"train\"]), batch_size))\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for i in t:\n",
        "    data = lm_datasets[\"train\"][i : i + batch_size]\n",
        "    data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "    student_out = student_model(**data, **forward_kwargs)\n",
        "    losses.append(student_out['loss'].detach().cpu().numpy())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    student_out['loss'].backward()\n",
        "    optimizer.step()\n",
        "    accuracy = th.eq(student_out['logits'].argmax(dim=2, keepdim = False).float(), data['labels']).float().mean()\n",
        "    accuracies.append(accuracy.detach().cpu().numpy())\n",
        "    loss = np.around(np.mean(losses[-100:]), 3)\n",
        "    accuracy = np.around(np.mean(accuracies[-100:]), 2)\n",
        "    t.set_description(\"Epoch: \"+str(epoch)+ \"Loss: \"+str(loss))\n",
        "  os.makedirs(root_folder+'bert_models', exist_ok=True)\n",
        "  th.save(student_model.state_dict(), root_folder+'bert_models/control_wikitext.pt')"
      ],
      "metadata": {
        "id": "JHktAtomHpTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset('glue', 'mrpc')"
      ],
      "metadata": {
        "id": "frjiJ3yETwoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "id": "mSBKGjx9T4KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets['train'][0]['sentence1'], datasets['train'][0]['sentence2'])"
      ],
      "metadata": {
        "id": "h7mVyjdDT5Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "mrpc_tok = datasets.map(lambda samples: tokenizer(samples['sentence1'], samples['sentence2'], padding='max_length', max_length=150),\n",
        "                        remove_columns = ['sentence1', 'sentence2','idx'],\n",
        "                        load_from_cache_file=False,\n",
        "                        )"
      ],
      "metadata": {
        "id": "zwe669VCUB8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "def filter_texts(examples):\n",
        "  examples[\"labels\"]= examples[\"label\"].copy()\n",
        "  examples.pop('label', None)\n",
        "  return examples\n",
        "mrpc_tok = mrpc_tok.map(\n",
        "    filter_texts,\n",
        "    batched=True,\n",
        "    batch_size= 1000,\n",
        "    num_proc=4,\n",
        "    load_from_cache_file=False)"
      ],
      "metadata": {
        "id": "5fZ9OZCuWUKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kd_loss import KnowledgeDistillationLoss\n",
        "from transformers import BertForNextSentencePrediction, BertForSequenceClassification\n",
        "teacher_model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "teacher_model.load_state_dict(th.load(root_folder+'bert_models/teacher_mrpc.pt'))\n",
        "student_model = BertForNextSentencePrediction(student_config).to(device)\n",
        "student_model.load_state_dict(th.load(root_folder+'bert_models/student_wikitext.pt'), strict=False)\n",
        "criterion = KnowledgeDistillationLoss(teacher_embd_dim, student_embd_dim, teacher_hddn_dim,student_hddn_dim,layer_mapping).to(device)"
      ],
      "metadata": {
        "id": "jLvTRVauWvxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "gc.collect()\n",
        "optimizer = optim.Adam(params=student_model.parameters(), lr = 5e-5, weight_decay=0.01)\n",
        "student_model.to(device)\n",
        "lr = 1e-4\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  mrpc[\"train\"].shuffle(load_from_cache_file=False)\n",
        "  t = tqdm(range(0, len(mrpc[\"train\"]), batch_size))\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for i in t:\n",
        "    data = mrpc[\"train\"][i : i + batch_size]\n",
        "    data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "    teacher_out = teacher_model(**data, **forward_kwargs)\n",
        "    student_out = student_model(**data, **forward_kwargs)\n",
        "    teacher_out['embeddings'] = teacher_model.get_input_embeddings().weight\n",
        "    student_out['embeddings'] = student_model.get_input_embeddings().weight\n",
        "    loss = criterion(teacher_out, student_out, penalize_prediction=True)\n",
        "    losses.append(loss.detach().cpu().numpy())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    accuracy = th.eq(student_out['logits'].argmax(dim=1, keepdim = False).float(), data['labels']).float().mean()\n",
        "    accuracies.append(accuracy.detach().cpu().numpy())\n",
        "    loss = np.around(np.mean(losses[-100:]), 3)\n",
        "    accuracy = np.around(np.mean(accuracies[-100:]), 2)\n",
        "    t.set_description(\"Epoch: \"+str(epoch)+ \"Loss: \"+str(loss)+\"Accuracy: \"+str(accuracy))\n",
        "  os.makedirs(root_folder+'bert_models', exist_ok=True)\n",
        "  th.save(student_model.state_dict(), root_folder+'bert_models/student_mrpc.pt')"
      ],
      "metadata": {
        "id": "_x0ElDO6YV5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "control_model = BertForNextSentencePrediction(student_config).to(device)\n",
        "control.load_state_dict(th.load(root_folder+'bert_models/control_wikitext.pt'), strict=False)\n",
        "gc.collect()\n",
        "control_model.train()\n",
        "optimizer = optim.Adam(params=student_model.parameters(), lr = lr, weight_decay=0.01)\n",
        "lr = 1e-4\n",
        "batch_size = 10\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  mrpc[\"train\"].shuffle()\n",
        "  t = tqdm(range(0, len(mrpc[\"train\"]), batch_size))\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for i in t:\n",
        "    data = mrpc[\"train\"][i : i + batch_size]\n",
        "    data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "    student_out = student_model(**data, **forward_kwargs)\n",
        "    losses.append(student_out['loss'].detach().cpu().numpy())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    student_out['loss'].backward()\n",
        "    optimizer.step()\n",
        "    accuracy = th.eq(student_out['logits'].argmax(dim=2, keepdim = False).float(), data['labels']).float().mean()\n",
        "    accuracies.append(accuracy.detach().cpu().numpy())\n",
        "    loss = np.around(np.mean(losses[-100:]), 3)\n",
        "    accuracy = np.around(np.mean(accuracies[-100:]), 2)\n",
        "    t.set_description(\"Epoch: \"+str(epoch)+ \"Loss: \"+str(loss))\n",
        "  os.makedirs(root_folder+'bert_models', exist_ok=True)\n",
        "  th.save(student_model.state_dict(), root_folder+'bert_models/control_wikitext.pt')"
      ],
      "metadata": {
        "id": "LGjMZhsoaHwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "tokenized_datasets = datasets.map(lambda samples: tokenizer(samples['text']), batched = True, num_proc = 4, remove_columns = [\"text\"])"
      ],
      "metadata": {
        "id": "IkVuC_FfbcJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "def group_texts(examples):\n",
        "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "  total_length = (total_length // block_size) * block_size\n",
        "  result = {\n",
        "      k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "      for k, t in concatenated_examples.items()\n",
        "  }\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result"
      ],
      "metadata": {
        "id": "mtY9g9U7bp31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size= 1000,\n",
        "    num_proc=4,\n",
        "    load_from_cache_file=False\n",
        ")"
      ],
      "metadata": {
        "id": "KKGygn5ecHRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "import gc\n",
        "gc.collect()\n",
        "teacher_model.train()\n",
        "optimizer = optim.Adam(params=student_model.parameters(), lr = lr, weight_decay=0.01)\n",
        "lr = 1e-4\n",
        "batch_size = 10\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  lm_datasets[\"train\"].shuffle()\n",
        "  t = tqdm(range(0, len(lm_datasets[\"train\"]), batch_size))\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for i in t:\n",
        "    data = lm_datasets[\"train\"][i:i+batch_size]\n",
        "    data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "    teacher_out = teacher_model(**data, **forward_kwargs)\n",
        "    losses.append(teacher_out['loss'].detach().cpu().numpy())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    teacher_out['loss'].backward()\n",
        "    optimizer.step()\n",
        "    accuracy = th.eq(teacher_out['logits'].argmax(dim=2, keepdim = False).float(), data['labels']).float().mean()\n",
        "    accuracies.append(accuracy.detach().cpu().numpy())\n",
        "    loss = np.around(np.mean(losses[-100:]), 3)\n",
        "    accuracy = np.around(np.mean(accuracies[-100:]), 2)\n",
        "    t.set_description(\"Epoch: \"+str(epoch)+ \"Loss: \"+str(loss)+\"Accuracy: \"+str(accuracy))"
      ],
      "metadata": {
        "id": "kHr-ApPjcPhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.eval()\n",
        "lm_datasets[\"validation\"].shuffle()\n",
        "t = tqdm(range(0, len(lm_datasets[\"validation\"]), batch_size))\n",
        "for i in t:\n",
        "  data = lm_datasets[\"validation\"][i:i+batch_size]\n",
        "  data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "  teacher_out = teacher_model(**data, **forward_kwargs)\n",
        "  losses.append(teacher_out['loss'].detach().cpu().numpy())\n",
        "\n",
        "  accuracy = th.eq(teacher_out['logits'].argmax(dim=2, keepdim = False).float(), data['labels']).float().mean()\n",
        "  accuracies.append(accuracy.detach().cpu().numpy())\n",
        "  loss = np.around(np.mean(losses), 3)\n",
        "  accuracy = np.around(np.mean(accuracies), 2)\n",
        "  t.set_description(\"Accuracy - \"+ \"Loss: \" + str(loss)+\" Accuracy: \"+str(accuracy))"
      ],
      "metadata": {
        "id": "JfHrpalxrgQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(root_folder+ 'bert_models', exist_ok = True)\n",
        "th.save(teacher_model.state_dict(), root_folder+ 'bert_models/teacher_wikitext.pt')"
      ],
      "metadata": {
        "id": "9HJ5zulKs7Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "datasets = load_dataset('glue', 'mrpc')\n",
        "mrpc_tok = datasets.map(lambda samples: tokenizer(samples['sentence1'], samples['sentence2'], padding = 'max_length', max_length = 150),\n",
        "                        remove_columns = ['sentence1', 'sentence2','idx'],\n",
        "                        load_from_cache_file=False)"
      ],
      "metadata": {
        "id": "4EIMmewKtCig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "def filter_texts(examples):\n",
        "  examples[\"labels\"] = examples[\"label\"].copy()\n",
        "  examples.pop('label', None)\n",
        "  return examples\n",
        "mrpc = mrpc_tok.map(\n",
        "    filter_texts,\n",
        "    batched= True,\n",
        "    batch_size = 1000,\n",
        "    num_proc = 4,\n",
        "    load_from_cache_file= False\n",
        ")"
      ],
      "metadata": {
        "id": "7KX4AJKLtr77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from transformers import BertForNextSentencePrediction, BertForSequenceClassification\n",
        "teacher_model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "xqZhIA7guBEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "import gc\n",
        "gc.collect()\n",
        "teacher_model.train()\n",
        "lr = 2e-5\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "optimizer = optim.Adam(params=teacher_model.parameters(), lr=lr, weight_decay=0.01)\n",
        "for epoch in range(epochs):\n",
        "  mrpc[\"train\"].shuffle(load_from_cache_file=False)\n",
        "  t = tqdm(range(0, len(mrpc[\"train\"]), batch_size))\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for i in t:\n",
        "    data = mrpc[\"train\"][i:i+batch_size]\n",
        "    data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "    teacher_out = teacher_model(**data, **forward_kwargs)\n",
        "    losses.append(teacher_out['loss'].detach().cpu().numpy())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    teacher_out['loss'].bachward()\n",
        "    optimizer.step()\n",
        "    accuracy = th.eq(teacher_out['logits'].argmax(dim=1, keepdim = False).float(), data['labels']).float().mean()\n",
        "    accuracies.append(accuracy.detach().cpu().numpy())\n",
        "    loss = np.around(np.mean(losses[-100:]), 3)\n",
        "    accuracy = np.around(np.mean(accuracies[-100:]), 2)\n",
        "    t.set_description(\"Epoch: \"+str(epoch)+ \"Loss: \"+str(loss)+\"Accuracy: \"+str(accuracy))"
      ],
      "metadata": {
        "id": "3gaiCGXluJtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.eval()\n",
        "mrpc[\"validation\"]\n",
        "t = tqdm(range(0, len(mrpc[\"validation\"]), batch_size))\n",
        "for i in t:\n",
        "  data = mrpc[\"validation\"][i:i+batch_size]\n",
        "  data = {k: th.tensor(v).to(device) for k, v in data.items()}\n",
        "  teacher_out = teacher_model(**data, **forward_kwargs)\n",
        "  losses.append(teacher_out['loss'].detach().cpu().numpy())\n",
        "\n",
        "  accuracy = th.eq(teacher_out['logits'].argmax(dim=1, keepdim = False).float(), data['labels']).float().mean()\n",
        "  accuracies.append(accuracy.detach().cpu().numpy())\n",
        "  loss = np.around(np.mean(losses), 3)\n",
        "  accuracy = np.around(np.mean(accuracies), 2)\n",
        "  t.set_description(\"Validatoin - \"+ \"Loss: \" + str(loss)+\" Accuracy: \"+str(accuracy))\n"
      ],
      "metadata": {
        "id": "Umx8ea5qvZon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(root_folder+ 'bert_models', exist_ok = True)\n",
        "th.save(teacher_model.state_dict(), root_folder+ 'bert_models/teacher_mrpc.pt')"
      ],
      "metadata": {
        "id": "3GaGURdGvyzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}