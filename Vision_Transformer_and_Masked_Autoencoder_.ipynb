{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66MZ8Px-l-KI"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install einops"
      ],
      "metadata": {
        "id": "8tASsFi7h5JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from seaborn\n",
        "seaborn.set()\n",
        "\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import einops\n",
        "import pickle\n",
        "import os\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "OYTHN6Yqh7CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7AcXry77iZiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"...\"\n",
        "os.makedirs(root_folder, exist_ok=True)\n",
        "os.chdir(root_folder)"
      ],
      "metadata": {
        "id": "WO1Qf2TAibS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O '/content/autograder_student.pt' '...'\n",
        "!wget -O '/content/test_reference.pt' '...'\n",
        "\n",
        "test_data = torch.load('/content/test_reference.pt')\n",
        "auto_grader_data = torch.load('/content/autograder_student.pt')\n",
        "auto_grader_data['output'] = {}"
      ],
      "metadata": {
        "id": "vEKUEWBaikTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_auto_grader_data():\n",
        "  torch.save(\n",
        "      {'output': auto_grader_data['output']},\n",
        "      'autograder.pt'\n",
        "  )\n",
        "\n",
        "def rel_error(x, y):\n",
        "  return torch.max(\n",
        "      torch.abs(x - y)\n",
        "       / (torch.maximum(torch.tensor(1e-8), torch.abs(x) + torch.abs(y)))\n",
        "  ).item()\n",
        "\n",
        "def check_error(name, x, y, tol=1e-3):\n",
        "  error = rel_error(x, y)\n",
        "  if error > tol:\n",
        "    print(f'The relative error for {name} is {error}, should be smaller than {tol}')\n",
        "  else:\n",
        "    print(f'The relative error for {name} is {error}')\n",
        "\n",
        "def check_acc(acc, threshold):\n",
        "  if acc > threshold:\n",
        "    print(f'The accuracy {acc} should >= threshold accuracy {threshold}')\n",
        "  else:\n",
        "    print(f'The accuracy is {acc} is better than threshold accuracy {threshold}')\n",
        "\n",
        "def patchify(images, patch_size=4):\n",
        "  #...\n",
        "  raise NotImplementedError\n",
        "\n",
        "def unpatchify(patches, patch_size=4):\n",
        "  #...\n",
        "  raise NotImplementedError"
      ],
      "metadata": {
        "id": "FmbNP8XLi5Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = test_data['input']['patchify']\n",
        "y = test_data['output']['patchify']\n",
        "check_error('patchify', patchify(x), y)\n",
        "\n",
        "x = auto_grader_data['input']['patchify']\n",
        "auto_grader_data['output']['patchify'] = patchify(x)\n",
        "save_auto_grader_data()\n",
        "\n",
        "x = test_data['input']['unpatchify']\n",
        "y = test_data['output']['unpatchify']\n",
        "check_error('unpatchify', unpatchify(x), y)\n",
        "\n",
        "x = auto_grader_data['input']['unpatchify']\n",
        "auto_grader_data['output']['unpatchify'] = unpatchify(x)\n",
        "\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "y1SEDQoRtf9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedding_dim=256, n_heads=4, n_layers=4, feedforward_dim=1024):\n",
        "    super().__init__()\n",
        "    self.embedding_dim =embedding_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.n_heads = n_heads\n",
        "    self.feedforward_dim = feedforward_dim\n",
        "    self.transformer = nn.TransformerEncoder(\n",
        "        nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead= self.n_heads,\n",
        "            dim_feedforward=self.feedforward_dim,\n",
        "            activation= F.gelu,\n",
        "            batch_first=True,\n",
        "            dropout=0.0\n",
        "        ),\n",
        "        num_layers=n_layers,\n",
        "    )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.transformer(x)\n",
        "\n",
        "class ClassificationViT(nn.Module):\n",
        "  def __init__(self, n_classes, embedding_dim=256, patch_size=4, num_patches=8):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.transformer = Transformer(embedding_dim)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
        "    self.position_encoding = nn.Parameter(\n",
        "        torch.randn(1, num_patches * num_patches + 1, embedding_dim) * 0.02\n",
        "        )\n",
        "    self.patch_projection = nn.Linear(patch_size * patch_size * 3, embedding_dim)\n",
        "\n",
        "    self.output_head = nn.Sequential(\n",
        "        nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, images):\n",
        "    # ...\n",
        "    raise NotImplementedError\n",
        "\n"
      ],
      "metadata": {
        "id": "Ix0mFn5JXH63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationViT(10)\n",
        "model.load_state_dict(test_data['weights']['ClassificationViT'])\n",
        "x = test_data['input']['ClassificationViT.forward']\n",
        "y = model.forward(x)\n",
        "check_error('ClassificationViT.forward', y, test_data['output']['ClassificationViT.forward'])\n",
        "\n",
        "model.load_state_dict(auto_grader_data['weights']['ClassificationViT'])\n",
        "x = auto_grader_data['input']['ClassificationViT.forward']\n",
        "y = model.forward(x)\n",
        "check_error('ClassificationViT.forward', y, test_data['output']['ClassificationViT.forward'])\n",
        "\n",
        "model.load_state_dict(auto_grader_data['weights']['ClassificationViT'])\n",
        "x = auto_grader_data['input']['ClassificationViT.forward']\n",
        "y = model.forward(x)\n",
        "auto_grader_data['output']['ClassificationViT.forward'] = y\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "25MWONHJZDAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding = 4),\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='/content/data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/content/data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "CKUiz6QgaC6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationViT(10)\n",
        "model.to(torch_device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.95), weight_decay=1e-9)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 10\n",
        "train_logfreq = 100\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "\n",
        "epoch_iteration = trange(num_epochs)\n",
        "for epoch in epoch_iteration:\n",
        "  data_iterator = tqdm(trainloader)\n",
        "  for x, y in data_iterator:\n",
        "    total_steps += 1\n",
        "    x, y = x.to(torch_device), y.to(torch_device)\n",
        "    logits = model(x)\n",
        "    loss = torch.mean(F.cross_entropy(logits, y))\n",
        "    accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    data_iterator.set_postfix(loss=loss.item(), train_acc = accuracy.item())\n",
        "\n",
        "    if total_steps % train_logfreq == 0:\n",
        "      losses.append(loss.item())\n",
        "      train_acc.append(accuracy.item())\n",
        "\n",
        "  val_acc = []\n",
        "  model.eval()\n",
        "  for x, y in testloader:\n",
        "    x, y = x.to(torch_device), y.to(torch_device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "    accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "    val_acc.append(accuracy.item())\n",
        "  model.train()\n",
        "\n",
        "  all_val_acc.append(np.mean(val_acc))\n",
        "  if np.mean(val_acc) > best_val_acc:\n",
        "    best_val_acc = np.mean(val_acc)\n",
        "\n",
        "  epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Val Accuracy')"
      ],
      "metadata": {
        "id": "szDGh9XydmPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_grader_data['output']['vit_acc'] = best_val_acc\n",
        "save_auto_grader_data()\n",
        "check_acc(best_val_acc, threshold=0.65)"
      ],
      "metadata": {
        "id": "X5NTEDbfuF-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_sequence(x, ids):\n",
        "  if len(x.shape) == 3:\n",
        "    ids = ids.unsqueeze(-1).expand(-1, -1, x.shape[-1])\n",
        "  return torch.take_along_dim(x, ids, dim=1)\n",
        "\n",
        "def random_masking(x, keep_length, ids_shuffle):\n",
        "  # ...\n",
        "  raise NotImplementedError\n",
        "\n",
        "def restore_masked(kept_x, masked_x, ids_restore):\n",
        "  # ...\n",
        "  raise NotImplementedError"
      ],
      "metadata": {
        "id": "xiVVtDJmuPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, ids_shulle = test_data['input']['random_masking']\n",
        "kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n",
        "kept_t, mask_t, ids_restore_t = test_data['output']['random_masking']\n",
        "check_error('random_masking: kept', kept, kept_t)\n",
        "check_error('random_masking: mask', mask, mask_t)\n",
        "check_error('random_masking: ids_restore', ids_restore, ids_restore_t)\n",
        "\n",
        "x, ids_shuffle = auto_grader_data['input']['random_masking']\n",
        "kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n",
        "auto_grader_data['output']['random_masking'] = (kept, mask, ids_restore)\n",
        "save_auto_grader_data()\n",
        "\n",
        "kept_x, masked_x, ids_restore = test_data['input']['restore_masked']\n",
        "restored = restore_masked(kept_x, masked_x, ids_restore)\n",
        "check_error('restore_masked', restored, test_data['output']['restore_masked'])\n",
        "\n",
        "kept_x, masked_x, ids_restore = auto_grader_data['input']['restore_masked']\n",
        "restored = restore_masked(kept_x, masked_x, ids_restore)\n",
        "auto_grader_data['output']['restore_masked'] = (kept, mask, ids_restore)\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "6qD8GwHeuxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedAutoEncoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder, encoder_embedding_dim=256,\n",
        "               decoder_embedding_dim=128, patch_size = 4, num_patches=8,\n",
        "               mask_ratio=0.75):\n",
        "    super().__init__()\n",
        "    self.encoder_embedding_dim = encoder_embedding_dim\n",
        "    self.decoder_embedding_dim = decoder_embedding_dim\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.mask_ratio = mask_ratio\n",
        "\n",
        "    self.masked_length = int(num_patches * num_patches * mask_ratio)\n",
        "    self.keep_length = num_patches * num_patches - self.masked_length\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "    self.encoder_input_projection = nn.Linear(patch_size * patch_size * 3, encoder_embedding_dim)\n",
        "    self.decoder_input_projection = nn.Linear(encoder_embedding_dim, decoder_embedding_dim)\n",
        "    self.decoder_output_projection = nn.Linear(decoder_embedding_dim, patch_size * patch_size * 3)\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1, encoder_embedding_dim) * 0.02)\n",
        "    self.encoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, encoder_embedding_dim) * 0.02)\n",
        "    self.decoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, decoder_embedding_dim) * 0.02)\n",
        "    self.masked_tokens = nn.Parameter(torch.randn(1, 1, decoder_embedding_dim) * 0.02)\n",
        "\n",
        "  def forward_encoder(self, images, ids_shuffle=None):\n",
        "    batch_size = images.shape[0]\n",
        "    if ids_shuffle is None:\n",
        "      ids_shuffle = torch.argsort(\n",
        "          torch.rand(\n",
        "              (batch_size, self.num_patches * self.num_patches),\n",
        "              device=images.device\n",
        "          ),\n",
        "          dim= 1\n",
        "      )\n",
        "      # ...\n",
        "      raise NotImplementedError\n",
        "\n",
        "def forward_decoder(self, encoder_embeddings, ids_restore):\n",
        "  # ...\n",
        "  raise NotImplementedError\n",
        "\n",
        "def forward(self, images):\n",
        "  encoder_output, mask, ids_restore = self.forward_encoder(images)\n",
        "  decoder_output = self.forward_decoder(encoder_output, ids_restore)\n",
        "  return decoder_output, mask\n",
        "\n",
        "def forward_encoder_representation(self, images):\n",
        "  #...\n",
        "  raise NotImplementedError"
      ],
      "metadata": {
        "id": "9TtL54nHwyMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim = 256, n_layers= 4),\n",
        "    Transformer(embedding_dim = 128, n_layers= 2),\n",
        ")\n",
        "\n",
        "model.load_state_dict(test_data['weights']['MaskedAutoEncoder'])\n",
        "images, ids_shuffle = test_data['input']['MaskedAutoEncoder.forward_encoder']\n",
        "encoder_embeddings_t, mask_t, ids_restore_t = test_data['output']['MaskedAutoEncoder.forward_encoder']\n",
        "encoder_embeddings, mask, ids_restore = model.forward_encoder(\n",
        "    images, ids_shuffle\n",
        "    )\n",
        "\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder: encoder_embeddings',\n",
        "    encoder_embeddings, encoder_embeddings_t\n",
        "    )\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder: mask',\n",
        "    mask, mask_t\n",
        "    )\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder: ids_restore',\n",
        "    ids_restore, ids_restore_t\n",
        "    )\n",
        "\n",
        "encoder_embeddings, ids_restore = test_data['input']['MaskedAutoEncoder.forward_decoder']\n",
        "decoder_output_t = test_data['output']['MaskedAutoEncoder.forward_decoder']\n",
        "decoder_output = model.forward_decoder(encoder_embeddings, ids_restore)\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_decoder',\n",
        "    decoder_output,\n",
        "    decoder_output_t\n",
        ")\n",
        "\n",
        "images = test_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n",
        "encoder_representations_t = test_data['output']['MaskedAutoEncoder.forward_encoder_representation']\n",
        "encoder_representations = model.forward_encoder_representation(images)\n",
        "check_error(\n",
        "    'MaskedAutoEncoder.forward_encoder_representation',\n",
        "    encoder_representations,\n",
        "    encoder_representations_t\n",
        ")\n",
        "\n",
        "model = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim = 256, n_layers= 4),\n",
        "    Transformer(embedding_dim = 128, n_layers= 2),\n",
        ")\n",
        "model.load_state_dict(auto_grader_data['weights']['MaskedAutoEncoder'])\n",
        "images, ids_shuffle = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder']\n",
        "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder'] = model.forward_encoder(\n",
        "    images, ids_shuffle\n",
        "    )\n",
        "\n",
        "encoder_embeddings, ids_restore = auto_grader_data['input']['MaskedAutoEncoder.forward_decoder']\n",
        "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder'] = model.forward_decoder(encoder_embeddings, ids_restore)\n",
        "\n",
        "images = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n",
        "auto_grader_data['output']['MaskedAutoEncoder.forward_encoder_representation'] = model.forward_encoder_representation(images)\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "Zn4wZPEI5xmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim = 256, n_layers= 4),\n",
        "    Transformer(embedding_dim = 128, n_layers= 2),\n",
        ")\n",
        "model.to(torch_device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=0.05)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 20\n",
        "train_logfreq = 100\n",
        "\n",
        "losses = []\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "  data_iterator = tqdm(trainloader)\n",
        "  for x, y in data_iterator:\n",
        "    total_steps += 1\n",
        "    x = x.to(torch_device)\n",
        "    image_patches = patchify(x)\n",
        "    predicted_patches, mask = model(x)\n",
        "    loss = torch.sum(torch.mean(torch.square(image_patches - predicted_patches), dim=-1)* mask) / mask.sum()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    data_iterator.set_postfix(loss=loss.item())\n",
        "    if total_steps % train_logfreq == 0:\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  torch.save(model.state_dict(), os.path.join(root_folder, \"mae_pretrained.pt\"))\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('MAE Train Loss')"
      ],
      "metadata": {
        "id": "fiMU4PM79D_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationMAE(nn.Module):\n",
        "  def __init__(self, n_classes, mae, embedding_dim=256, detach=False):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.mae = mae\n",
        "    self.output_head = nn.Sequential(\n",
        "        nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n",
        "    )\n",
        "    self.detach = detach\n",
        "\n",
        "  def forward(self, images):\n",
        "    #...\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "r-5fkzIpCdO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationMAE(\n",
        "    10,\n",
        "    MaskedAutoEncoder(\n",
        "        Transformer(embedding_dim = 256, n_layers= 4),\n",
        "        Transformer(embedding_dim = 128, n_layers= 2),\n",
        "    )\n",
        "  )\n",
        "\n",
        "model.load_state_dict(test_data['weights']['ClassificationMAE'])\n",
        "\n",
        "check_error(\n",
        "    'ClassificationMAE.forward',\n",
        "    model(test_data['input']['ClassificationMAE.forward']),\n",
        "    test_data['output']['ClassificationMAE.forward']\n",
        ")\n",
        "\n",
        "model = ClassificationMAE(\n",
        "    10,\n",
        "    MaskedAutoEncoder(\n",
        "        Transformer(embedding_dim = 256, n_layers= 4),\n",
        "        Transformer(embedding_dim = 128, n_layers= 2),\n",
        "    )\n",
        ")\n",
        "\n",
        "model.load_state_dict(auto_grader_data['weights']['ClassificationMAE'])\n",
        "auto_grader_data['output']['ClassificationMAE.forward'] = model(\n",
        "    auto_grader_data['input']['ClassificationMAE.forward']\n",
        "    )\n",
        "save_auto_grader_data()"
      ],
      "metadata": {
        "id": "IT_zlbeZC4FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = MaskedAutoEncoder(\n",
        "    Transformer(embedding_dim = 256, n_layers= 4),\n",
        "    Transformer(embedding_dim = 128, n_layers= 2),\n",
        ")\n",
        "mae.load_state_dict(torch.load(os.path.join(root_folder, \"mae_pretrained.pt\")))"
      ],
      "metadata": {
        "id": "hDlW1MFVGNZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationMAE(10, mae, detach=True)\n",
        "model.to(torch_device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 20\n",
        "train_logfreq = 100\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "  data_iterator = tqdm(trainloader)\n",
        "  for x, y in data_iterator:\n",
        "    total_steps =+ 1\n",
        "    x, y = x.to(torch_device), y.to(torch_device)\n",
        "    logits = model(x)\n",
        "    loss = torch.mean(F.cross_entropy(logits, y))\n",
        "    accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    data_iterator.set_postfix(loss=loss.item(), train_acc = accuracy.item())\n",
        "\n",
        "    if total_steps % train_logfreq == 0:\n",
        "      losses.append(loss.item())\n",
        "      train_acc.append(accuracy.item())\n",
        "\n",
        "  val_acc = []\n",
        "  medal.eval()\n",
        "  for x, y in testloader:\n",
        "    x, y = x.to(torch_device), y.to(torch_device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "    accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
        "    val_acc.append(accuracy.item())\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  all_val_acc.append(np.mean(val_acc))\n",
        "\n",
        "  if np.mean(val_acc) > best_val_acc:\n",
        "    best_val_acc = np.mean(val_acc)\n",
        "\n",
        "  epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Linear Classification Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Linear Classification Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Linear Classification Val Accuracy')"
      ],
      "metadata": {
        "id": "0HAfQnviGd1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_grader_data['output']['mae_linear_acc'] = best_val_acc\n",
        "save_auto_grader_data()\n",
        "check_acc(best_val_acc, threshold=0.30)"
      ],
      "metadata": {
        "id": "hWdzZH0eKQaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationMAE(10, mae, detach=False)\n",
        "model.to(torch_device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n",
        "\n",
        "total_steps = 0\n",
        "num_epochs = 20\n",
        "train_logfreq = 100\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "  data_iterator = tqdm(trainloader)\n",
        "  for x, y in data_iterator:\n",
        "    total_steps += 1\n",
        "    x, y = x.to(torch_device), y.to(torch_device)\n",
        "    logits = model(x)\n",
        "    loss = torch.mean(F.cross_entropy(logits, y))\n",
        "    accuracy = torch.mean((torch.argmax(logits, dim=-1) ==y).float())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
        "\n",
        "    if total_steps % train_logfreq == 0:\n",
        "      losses.append(loss.item())\n",
        "      train_acc.append(accuracy.item())\n",
        "\n",
        "  vall_acc = []\n",
        "  model.eval()\n",
        "  for x, y in testloader:\n",
        "    x, y = x.to(torch_device), y.to(torch_device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "    accuracy = torch.mean((torch.argmax(logits, dim=-1) ==y).float())\n",
        "    val_acc.append(accuracy.item())\n",
        "  model.train()\n",
        "\n",
        "  all_val_acc.append(np.mean(val_acc))\n",
        "\n",
        "  if np.mean(val_acc) > best_val_acc:\n",
        "    best_val_acc = np.mean(val_acc)\n",
        "\n",
        "  epoch_iterator.set_postfix(val_acc = np.mean(val_acc), best_val_acc = best_val_acc)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Finetune Classification Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Finetune Classification Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Finetune Classification Val Accuracy')"
      ],
      "metadata": {
        "id": "MWI2MNAmKhPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_grader_data['output']['mae_finetune_acc'] = best_val_acc\n",
        "save_auto_grader_data()\n",
        "check_acc(best_val_acc, threshold=0.70)"
      ],
      "metadata": {
        "id": "IdldmbQYMuy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}