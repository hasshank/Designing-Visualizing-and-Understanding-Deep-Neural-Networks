{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-IbKpx-eC2X"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Callable, Tuple\n",
        "\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "from transformer_utils import ApplyAttentionMask\n",
        "\n",
        "class AttentionQKV(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.apply_mask = ApplyAttentionMask()\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "\n",
        "      # ...\n",
        "      # https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "      key_dim = th.tensor(keys.shape[-1], dtype=th.float32)\n",
        "      similarity = # ...\n",
        "\n",
        "      masked_similarity = self.apply_mask(similarity, mask)\n",
        "      weights = # ...\n",
        "      output = # ...\n",
        "\n",
        "      return output, attention_weights\n",
        "\n",
        "class MultiHeadProjection(nn.Module):\n",
        "    def __init__(self, n_heads, feature_sizes):\n",
        "        super().__init__()\n",
        "        self.attention_map = AttentionQKV()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        for size in feature_sizes:\n",
        "            assert size % self.n_heads == 0, 'Shape of feature input must be divisible by n_heads'\n",
        "\n",
        "    def forward(self, inputs, mask=None):\n",
        "      queries, keys, values = inputs\n",
        "      queries_split = self._split_heads(queries)\n",
        "      keys_split = self._split_heads(keys)\n",
        "      values_split = self._split_heads(values)\n",
        "\n",
        "      attention_output_split = self.attention_map(queries_split, keys_split, values_split, mask=mask)\n",
        "\n",
        "      output = self._combine_heads(attention_output_split)\n",
        "\n",
        "      return output\n",
        "\n",
        "    def _split_heads(self, tensor):\n",
        "      assert len(tensor.shape) == 3\n",
        "\n",
        "      # ...\n",
        "\n",
        "      batch_size, tensorlen = tensor.shape[0], tensor.shape[1]\n",
        "      feature_size = tensor.shape[2]\n",
        "\n",
        "      new_feature_size = # ...\n",
        "      tensor = # ...\n",
        "\n",
        "      return tensor\n",
        "\n",
        "    def _combine_heads(self, tensor):\n",
        "      assert len(tensor.shape) == 4\n",
        "\n",
        "      # ...\n",
        "\n",
        "      tensor =\n",
        "\n",
        "      batch_size, tensorlen = tensor.shape[0], tensor.shape[1]\n",
        "      feature_size = tensor.shape[-1]\n",
        "\n",
        "      new_feature_size = # ...\n",
        "      tensor = # ...\n",
        "\n",
        "      return tensor\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, input_shapes):\n",
        "        super().__init__()\n",
        "        self.qa_channels, self.ma_channels = input_shapes\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.attention_layer = MultiHeadProjection(n_heads, (self.qa_channels, self.ma_channels))\n",
        "\n",
        "        assert self.qa_channels % self.n_heads == 0, and self.ma_channels % self.n_heads == 0 and \\\n",
        "                                            'Feature size must be divisible by n_heads'\n",
        "        assert self.qa_channels == self.qa_channels and 'Cannot combine tensors with different shapes'\n",
        "\n",
        "        self.query_layer = weight_norm(nn.Linear(self.qa_channels, self.qa_channels, bias = False))\n",
        "        self.key_layer = weight_norm(nn.Linear(self.qa_channels, self.qa_channels, bias = False))\n",
        "        self.value_layer = weight_norm(nn.Linear(self.ma_channels, self.ma_channels, bias = False))\n",
        "        self.output_layer = weight_norm(nn.Linear(self.qa_channels, self.qa_channels, bias = False))\n",
        "\n",
        "        def weights_init(m):\n",
        "          nn.init.xavier_uniform_(m.weight.data)\n",
        "        self.query_layer.apply(weights_init)\n",
        "        self.key_layer.apply(weights_init)\n",
        "        self.value_layer.apply(weights_init)\n",
        "        self.output_layer.apply(weights_init)\n",
        "\n",
        "      def forward(self, inputs, mask=None):\n",
        "        assert (isinstance(inputs, tuple) or isinstance(inputs, list)) and len(inputs) == 2 and \\\n",
        "                                                        'Must pass query and memory'\n",
        "        query_antecedent, memory_antecedent = inputs\n",
        "\n",
        "        q = self.query_layer(query_antecedent)\n",
        "        k = self.key_layer(memory_antecedent)\n",
        "        v = self.value_layer(memory_antecedent)\n",
        "\n",
        "        attention_output = self.attention_layer((q, k, v), mask=mask)\n",
        "        output = self.output_layer(attention_output)\n",
        "\n",
        "        return output\n",
        "\n",
        ""
      ]
    }
  ]
}